{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10523555,"sourceType":"datasetVersion","datasetId":6513061},{"sourceId":10523833,"sourceType":"datasetVersion","datasetId":6513240},{"sourceId":10618577,"sourceType":"datasetVersion","datasetId":6574452},{"sourceId":10632520,"sourceType":"datasetVersion","datasetId":6582826},{"sourceId":10632553,"sourceType":"datasetVersion","datasetId":6582848},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Training the model**\n\n## **Install dependencies**\n### Install Keras, KerasNLP, and other dependencies.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -q bitsandbytes\n%pip install -q transformers\n%pip install -q peft\n%pip install -q accelerate\n%pip install -q trl\n%pip install -q torch\n%pip install -q qdrant-client langchain pypdf sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:46:18.759019Z","iopub.execute_input":"2025-02-01T16:46:18.759398Z","iopub.status.idle":"2025-02-01T16:46:42.340318Z","shell.execute_reply.started":"2025-02-01T16:46:18.759364Z","shell.execute_reply":"2025-02-01T16:46:42.339073Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"!pip install langchain_community","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:46:42.341732Z","iopub.execute_input":"2025-02-01T16:46:42.342008Z","iopub.status.idle":"2025-02-01T16:46:45.809174Z","shell.execute_reply.started":"2025-02-01T16:46:42.341983Z","shell.execute_reply":"2025-02-01T16:46:45.808331Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.16)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\nRequirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.17)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.33)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.1)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.24.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (0.3.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (2.10.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (2.27.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## **Load all libraries**","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os, torch\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nfrom datasets import Dataset\nfrom IPython.display import Markdown, display\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader\nfrom langchain.vectorstores import Qdrant\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:46:45.811314Z","iopub.execute_input":"2025-02-01T16:46:45.811633Z","iopub.status.idle":"2025-02-01T16:46:45.818522Z","shell.execute_reply.started":"2025-02-01T16:46:45.811595Z","shell.execute_reply":"2025-02-01T16:46:45.817671Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## The code below configures a large language model (LLM) for inference with quantization techniques for efficiency. Here's a breakdown of what each part does:\n\n**Model Path and Quantization Configuration**\n\n1. **Model Path:** The `model` variable stores the path to a pre-trained causal language model (likely a 2-billion parameter model) on Kaggle Datasets.\n\n2. **BitsAndBytesConfig:** The `bnbConfig` object defines the configuration for quantization using the BitsAndBytes library. Here are the key arguments:\n    * `load_in_4bit (bool, optional)`: This argument enables 4-bit quantization, reducing memory usage by approximately fourfold compared to the original model.\n    * `bnb_4bit_quant_type (str, optional)`: This parameter specifies the type of 4-bit quantization to use. Here, it's set to `\"nf4\"`, a specific quantization format supported by BitsAndBytes.\n    * `bnb_4bit_compute_dtype (torch.dtype, optional)`: This argument defines the data type used for computations during inference. Here, it's set to `torch.bfloat16`, a lower-precision format that can improve speed on compatible hardware.\n\n**Loading Tokenizer and Model with Quantization**\n\n1. **AutoTokenizer:** The `AutoTokenizer.from_pretrained` function loads the tokenizer associated with the pre-trained model at the specified path (`model`). The `quantization_config` argument is crucial here. It tells the tokenizer to consider the quantization information (e.g., potential padding changes) while processing text.\n\n2. **AutoModelForCausalLM:** Similarly, `AutoModelForCausalLM.from_pretrained` loads the actual LLM model from the path (`model`). Again, the `device_map=\"auto\"` argument allows automatic device placement (CPU or GPU) and the `quantization_config` ensures the model is loaded with the 4-bit quantization configuration.\n\n**Overall, this code snippet aims to achieve two goals:**\n\n* **Load a pre-trained LLM:** It retrieves a pre-trained causal language model from the specified path.\n* **Enable Quantization for Efficiency:** By using the `BitsAndBytesConfig` and arguments during loading, the code configures the tokenizer and model to leverage 4-bit quantization for memory reduction and potentially faster inference on compatible hardware.\n","metadata":{}},{"cell_type":"markdown","source":"<h3><strong>Know More about <a href=\"https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2\">4-bit quantization</a></strong></h3>","metadata":{}},{"cell_type":"code","source":"#model = \"/kaggle/input/m/google/gemma/transformers/2b-it/2\" #Zafor158/lora-alpaca\nmodel=\"/kaggle/input/m/google/gemma/transformers/2b-it/2\"\n\nbnbConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model, quantization_config=bnbConfig, device_map=\"auto\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model,\n    device_map = \"auto\",\n    quantization_config=bnbConfig\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:46:45.819745Z","iopub.execute_input":"2025-02-01T16:46:45.820040Z","iopub.status.idle":"2025-02-01T16:46:53.810587Z","shell.execute_reply.started":"2025-02-01T16:46:45.820018Z","shell.execute_reply":"2025-02-01T16:46:53.809594Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4543e56f6104a14bf7753cf5d4fb41e"}},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"## **Test Model using a prompt**\n### The code below demonstrates how to use a large language model (LLM) for creative text generation. Here's a breakdown of what each part does:\n\n**Creating the Prompt**\n\n1. **System Response:** The code defines a variable `system` containing a message praising your Python coding skills.\n2. **User Request:** The `user` variable specifies the request to \"Write a Python code to display text in a star pattern.\"\n3. **Prompt Construction:** The `prompt` variable combines the system response, user request, and an AI response placeholder using f-strings.\n\n**Tokenization and Model Input Preparation**\n\n1. **Tokenizer:** The `tokenizer` likely refers to a pre-trained tokenizer function from the Hugging Face Transformers library. It converts the text in the prompt into numerical representations suitable for the LLM.\n2. **Tensor Conversion:** `.to(\"cuda\")` converts the tokenized prompt into a PyTorch tensor and moves it to the GPU (if available) for faster processing.\n\n**Model Generation**\n\n1. **Model Generation:** The `model.generate` function utilizes the LLM to generate text following the prompt. The provided arguments specify:\n    * `inputs`: The tokenized prompt as input.\n    * `num_return_sequences`: Set to 1, indicating only one generated sequence is desired.\n    * `max_new_tokens`: Limits the maximum number of tokens the model generates to 1000.\n\n**Decoding and Output**\n\n1. **Decoding:** The `tokenizer.decode` function converts the generated token sequence back into human-readable text.\n2. **Splitting and Markdown:** The code splits the generated text by \"AI:\" to extract the AI's response. Finally, it wraps the response in a Markdown object, likely for formatting purposes (not shown in the provided code).\n\n**Overall Functionality:**\n\nThis code snippet simulates a conversation where the user asks for Python code for a star pattern, and the LLM generates the code using the prompt and its knowledge.\n\n**Note:** The actual Python code for generating a star pattern is not included here, but the LLM would likely generate the code based on its training data.\n","metadata":{}},{"cell_type":"code","source":"system =  \"You are a skilled GPT for question answering.\"\nuser = \"What is drug?provide me some drug name and its uses\"\n\nprompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=1000)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nMarkdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:46:53.811429Z","iopub.execute_input":"2025-02-01T16:46:53.811637Z","iopub.status.idle":"2025-02-01T16:47:18.498152Z","shell.execute_reply.started":"2025-02-01T16:46:53.811619Z","shell.execute_reply":"2025-02-01T16:47:18.497322Z"},"trusted":true},"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 1103, in emit\n    stream.write(msg + self.terminator)\nValueError: I/O operation on closed file\nCall stack:\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n    yield self.process_one()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n    self.do_execute(\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-e451e58dcb38>\", line 6, in <cell line: 6>\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2860, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2970, in _call_one\n    return self.encode_plus(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3037, in encode_plus\n    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2751, in _get_padding_truncation_strategies\n    logger.warning(\nMessage: 'Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.'\nArguments: ()\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" \n\nSure, here's a comprehensive list of drugs and their uses:\n\n**1. Alcohol**\n\n- Uses:\n  - Moderate consumption (up to 1 drink per day for women and 2 drinks per day for men) can reduce the risk of heart disease, stroke, and some types of cancer.\n  - Excessive consumption can lead to liver damage, alcohol poisoning, and death.\n\n**2. Caffeine**\n\n- Uses:\n  - Stimulates the central nervous system, improving alertness, focus, and coordination.\n  - Moderate consumption (up to 400 milligrams per day) is safe for most adults.\n\n**3. Cannabis**\n\n- Uses:\n  - Reduces pain, anxiety, and nausea.\n  - Can be used for medical conditions such as epilepsy, glaucoma, and chronic pain.\n  - Cannabis use can also lead to addiction and respiratory problems.\n\n**4. Cocaine**\n\n- Uses:\n  - Highly addictive stimulant that can cause severe health problems, including heart disease, stroke, and respiratory failure.\n  - Use can also lead to psychological problems such as paranoia, hallucinations, and psychosis.\n\n**5. Heroin**\n\n- Uses:\n  - Highly addictive opioid that can cause respiratory depression, overdose, and death.\n  - Heroin use can also lead to addiction and withdrawal symptoms.\n\n**6. Marijuana**\n\n- Uses:\n  - Reduces pain, anxiety, and nausea.\n  - Can be used for medical conditions such as chronic pain, epilepsy, and glaucoma.\n  - Marijuana use can also lead to addiction and respiratory problems.\n\n**7. Prescription Drugs**\n\n- Uses:\n  - Are used under the supervision of a healthcare professional to treat specific medical conditions.\n  - Prescription drugs can be used to treat conditions such as pain, depression, anxiety, and cancer.\n\n**8. Over-the-Counter Drugs**\n\n- Uses:\n  - Are available without a prescription from a healthcare professional.\n  - Over-the-counter drugs can be used to treat minor illnesses and symptoms such as colds, flu, indigestion, and headaches.\n\n**9. Supplements**\n\n- Uses:\n  - Are taken orally to support overall health and well-being.\n  - Supplements can be used to improve digestion, boost the immune system, and reduce the risk of certain diseases.\n\n**10. Herbal Products**\n\n- Uses:\n  - Are derived from plants and are used for medicinal purposes.\n  - Herbal products can be used to treat a variety of health conditions, but it's important to consult with a healthcare professional before using them."},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:47:18.499038Z","iopub.execute_input":"2025-02-01T16:47:18.499384Z","iopub.status.idle":"2025-02-01T16:47:18.504906Z","shell.execute_reply.started":"2025-02-01T16:47:18.499351Z","shell.execute_reply":"2025-02-01T16:47:18.504290Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# **3. Fine Tune Model**","metadata":{}},{"cell_type":"markdown","source":"## **Load the dataset**","metadata":{}},{"cell_type":"code","source":"#data = pd.read_csv(\"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\")\ndata = pd.read_csv(\"/kaggle/input/modified-488-data/modified_dataset.csv\")\ndataset = Dataset.from_pandas(data)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:47:18.505742Z","iopub.execute_input":"2025-02-01T16:47:18.505975Z","iopub.status.idle":"2025-02-01T16:47:23.337691Z","shell.execute_reply.started":"2025-02-01T16:47:18.505946Z","shell.execute_reply":"2025-02-01T16:47:23.336807Z"},"trusted":true},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"   uniqueID Drug Name                                 Generic Name  \\\n0     94783   Abilify  aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]   \n1     94670   Abilify  aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]   \n2     94649   Abilify  aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]   \n3     94633   Abilify  aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]   \n4     95078   Abilify  aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]   \n\n                Drug Class                                               Uses  \\\n0  Atypical antipsychotics  Abilify is an antipsychotic medication. It wor...   \n1  Atypical antipsychotics  Abilify is an antipsychotic medication. It wor...   \n2  Atypical antipsychotics  Abilify is an antipsychotic medication. It wor...   \n3  Atypical antipsychotics  Abilify is an antipsychotic medication. It wor...   \n4  Atypical antipsychotics  Abilify is an antipsychotic medication. It wor...   \n\n                                            Warnings  \\\n0  Abilify is not approved for use in older adult...   \n1  Abilify is not approved for use in older adult...   \n2  Abilify is not approved for use in older adult...   \n3  Abilify is not approved for use in older adult...   \n4  Abilify is not approved for use in older adult...   \n\n                                Pre-Use Instructions  \\\n0  You should not take Abilify if you are allergi...   \n1  You should not take Abilify if you are allergi...   \n2  You should not take Abilify if you are allergi...   \n3  You should not take Abilify if you are allergi...   \n4  You should not take Abilify if you are allergi...   \n\n                                              Dosage  \\\n0  Take Abilify exactly as prescribed by your doc...   \n1  Take Abilify exactly as prescribed by your doc...   \n2  Take Abilify exactly as prescribed by your doc...   \n3  Take Abilify exactly as prescribed by your doc...   \n4  Take Abilify exactly as prescribed by your doc...   \n\n                                        Side Effects  \\\n0  Get emergency medical help if you havesigns of...   \n1  Get emergency medical help if you havesigns of...   \n2  Get emergency medical help if you havesigns of...   \n3  Get emergency medical help if you havesigns of...   \n4  Get emergency medical help if you havesigns of...   \n\n                  condition  \\\n0                Depression   \n1  Major Depressive Disorde   \n2                Depression   \n3           Bipolar Disorde   \n4           Bipolar Disorde   \n\n                                              review  rating       date  \\\n0  \"For the past 20 years I have been on antidepr...      10   8-Aug-13   \n1  \"5 days - from non functioning depressed to li...      10  15-Sep-15   \n2  \"On 2 mg. It made me feel empty inside. Like t...       5  29-Feb-16   \n3  \"Took Abilify along with Lithium.  Abilify is ...       7  15-May-16   \n4  \"I am bipolar and have swung into a massive de...       9  14-Jun-09   \n\n   usefulCount  \n0          162  \n1           23  \n2           19  \n3           10  \n4           81  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniqueID</th>\n      <th>Drug Name</th>\n      <th>Generic Name</th>\n      <th>Drug Class</th>\n      <th>Uses</th>\n      <th>Warnings</th>\n      <th>Pre-Use Instructions</th>\n      <th>Dosage</th>\n      <th>Side Effects</th>\n      <th>condition</th>\n      <th>review</th>\n      <th>rating</th>\n      <th>date</th>\n      <th>usefulCount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>94783</td>\n      <td>Abilify</td>\n      <td>aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]</td>\n      <td>Atypical antipsychotics</td>\n      <td>Abilify is an antipsychotic medication. It wor...</td>\n      <td>Abilify is not approved for use in older adult...</td>\n      <td>You should not take Abilify if you are allergi...</td>\n      <td>Take Abilify exactly as prescribed by your doc...</td>\n      <td>Get emergency medical help if you havesigns of...</td>\n      <td>Depression</td>\n      <td>\"For the past 20 years I have been on antidepr...</td>\n      <td>10</td>\n      <td>8-Aug-13</td>\n      <td>162</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>94670</td>\n      <td>Abilify</td>\n      <td>aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]</td>\n      <td>Atypical antipsychotics</td>\n      <td>Abilify is an antipsychotic medication. It wor...</td>\n      <td>Abilify is not approved for use in older adult...</td>\n      <td>You should not take Abilify if you are allergi...</td>\n      <td>Take Abilify exactly as prescribed by your doc...</td>\n      <td>Get emergency medical help if you havesigns of...</td>\n      <td>Major Depressive Disorde</td>\n      <td>\"5 days - from non functioning depressed to li...</td>\n      <td>10</td>\n      <td>15-Sep-15</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>94649</td>\n      <td>Abilify</td>\n      <td>aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]</td>\n      <td>Atypical antipsychotics</td>\n      <td>Abilify is an antipsychotic medication. It wor...</td>\n      <td>Abilify is not approved for use in older adult...</td>\n      <td>You should not take Abilify if you are allergi...</td>\n      <td>Take Abilify exactly as prescribed by your doc...</td>\n      <td>Get emergency medical help if you havesigns of...</td>\n      <td>Depression</td>\n      <td>\"On 2 mg. It made me feel empty inside. Like t...</td>\n      <td>5</td>\n      <td>29-Feb-16</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>94633</td>\n      <td>Abilify</td>\n      <td>aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]</td>\n      <td>Atypical antipsychotics</td>\n      <td>Abilify is an antipsychotic medication. It wor...</td>\n      <td>Abilify is not approved for use in older adult...</td>\n      <td>You should not take Abilify if you are allergi...</td>\n      <td>Take Abilify exactly as prescribed by your doc...</td>\n      <td>Get emergency medical help if you havesigns of...</td>\n      <td>Bipolar Disorde</td>\n      <td>\"Took Abilify along with Lithium.  Abilify is ...</td>\n      <td>7</td>\n      <td>15-May-16</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>95078</td>\n      <td>Abilify</td>\n      <td>aripiprazole  (oral) [  AR-i-PIP-ra-zole  ]</td>\n      <td>Atypical antipsychotics</td>\n      <td>Abilify is an antipsychotic medication. It wor...</td>\n      <td>Abilify is not approved for use in older adult...</td>\n      <td>You should not take Abilify if you are allergi...</td>\n      <td>Take Abilify exactly as prescribed by your doc...</td>\n      <td>Get emergency medical help if you havesigns of...</td>\n      <td>Bipolar Disorde</td>\n      <td>\"I am bipolar and have swung into a massive de...</td>\n      <td>9</td>\n      <td>14-Jun-09</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"## **Define a formatting function for the model output.**","metadata":{}},{"cell_type":"code","source":"# def formatting_func(example):\n#     template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n#     line = template.format(instruction=example['Question'], response=example['Answer'])\n#     return [line]\n    \ndef formatting_func(example):\n    template = (\n        \"Drug Name: {Drug Name}\\n\"\n        \"Generic Name: {Generic Name}\\n\"\n        \"Drug Class: {Drug Class}\\n\"\n        \"Uses: {Uses}\\n\"\n        \"Warnings: {Warnings}\\n\"\n        \"Pre-Use Instructions: {Pre-Use Instructions}\\n\"\n        \"Dosage: {Dosage}\\n\"\n        \"Side Effects: {Side Effects}\\n\"\n        \"Condition: {condition}\\n\"\n        \"Review: {review}\\n\"\n        \"Rating: {rating}\\n\"\n        \"Date: {date}\\n\"\n        \"Useful Count: {usefulCount}\"\n    )\n    line = template.format(\n        **{\n            key: example.get(key, \"N/A\") for key in [\n                \"Drug Name\", \"Generic Name\", \"Drug Class\", \"Uses\", \"Warnings\", \n                \"Pre-Use Instructions\", \"Dosage\", \"Side Effects\", \"condition\", \n                \"review\", \"rating\", \"date\", \"usefulCount\"\n            ]\n        }\n    )\n    return [line]\n\n# def formatting_func(example):\n#     template = (\n#         \"Drug Name: {DrugName}\\n\"\n#         \"Generic Name: {Drug_Generic_name}\\n\"\n#         \"Class: {Drug_class}\\n\"\n#         \"Uses: {Uses}\\n\"\n#         \"Warnings: {Warnings}\\n\"\n#         \"Pre-use Instructions: {PreUseInstructions}\\n\"\n#         \"Dosage: {Dosage}\\n\"\n#         \"Side Effects: {Side_effects}\\n\"\n#         \"Interactions: {Interactions}\\n\"\n#         \"User Ratings: {User_Ratings}\\n\"\n#     )\n    \n#     # Formatting each field and providing a default if missing\n#     line = template.format(\n#         DrugName=example.get('DrugName', 'Not provided'),\n#         Drug_Generic_name=example.get('Drug_Generic_name', 'Not provided'),\n#         Drug_class=example.get('Drug_class', 'Not provided'),\n#         Uses=example.get('Uses', 'Not provided'),\n#         Warnings=example.get('Warnings', 'Not provided'),\n#         PreUseInstructions=example.get('PreUseInstructions', 'Not provided'),\n#         Dosage=example.get('Dosage', 'Not provided'),\n#         Side_effects=example.get('Side effects', 'Not provided'),\n#         Interactions=example.get('Interactions', 'Not provided'),\n#         User_Ratings=example.get('User_Ratings', 'Not provided')\n#     )\n    \n#     return [line]  # Return as a list containing the formatted string\n","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:47:23.340198Z","iopub.execute_input":"2025-02-01T16:47:23.340438Z","iopub.status.idle":"2025-02-01T16:47:23.345302Z","shell.execute_reply.started":"2025-02-01T16:47:23.340418Z","shell.execute_reply":"2025-02-01T16:47:23.344435Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:47:23.347187Z","iopub.execute_input":"2025-02-01T16:47:23.347569Z","iopub.status.idle":"2025-02-01T16:47:23.362594Z","shell.execute_reply.started":"2025-02-01T16:47:23.347534Z","shell.execute_reply":"2025-02-01T16:47:23.361870Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r = 32,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type = \"CAUSAL_LM\",\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:47:23.363310Z","iopub.execute_input":"2025-02-01T16:47:23.363559Z","iopub.status.idle":"2025-02-01T16:47:23.373921Z","shell.execute_reply.started":"2025-02-01T16:47:23.363531Z","shell.execute_reply":"2025-02-01T16:47:23.373067Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### The code above defines a configuration object called `lora_config` for a technique called LoRA (Low-Rank Adaptation). Here's a breakdown of what each parameter does:\n\n**LoRA - Low-Rank Adaptation**\n\nLoRA is a technique used to fine-tune large language models (LLMs) more efficiently. It allows you to adapt pre-trained models to new tasks with minimal memory and computational cost compared to traditional fine-tuning.\n\n**LoraConfig Parameters:**\n\n* **r (int):** This parameter defines the rank of the low-rank decomposition used in LoRA. It controls the trade-off between accuracy and memory usage. A lower value of `r` uses less memory but might lead to slightly lower accuracy. The default value is typically 8, as set in our code.\n\n* **target_modules (List[str]):** This list specifies the Transformer layers where LoRA will be applied. The provided configuration targets several key projection layers within the Transformer architecture:\n    * `q_proj`: Query projection\n    * `o_proj`: Output projection\n    * `k_proj`: Key projection\n    * `v_proj`: Value projection\n    * `gate_proj`: Gate projection (used in attention layers)\n    * `up_proj`: Upsampling projection (used in some encoder-decoder architectures)\n    * `down_proj`: Downsampling projection (used in some encoder-decoder architectures)\n\nBy applying LoRA to these projection layers, the model can learn task-specific adaptations without modifying the original large model weights significantly.\n\n* **task_type (str, optional):** This parameter specifies the type of task you're fine-tuning the model for. While not used in this specific configuration, some libraries might leverage this information to optimize LoRA for specific task categories (e.g., \"CAUSAL_LM\" for causal language modeling).\n\n**In summary:**\n\nThis configuration defines how LoRA will be applied to a pre-trained model for fine-tuning. It specifies the rank of the decomposition (memory usage) and the target layers within the Transformer architecture where LoRA will be used to adapt the model to a new task.\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:47:23.374898Z","iopub.execute_input":"2025-02-01T16:47:23.375109Z","iopub.status.idle":"2025-02-01T16:47:26.839867Z","shell.execute_reply.started":"2025-02-01T16:47:23.375081Z","shell.execute_reply":"2025-02-01T16:47:26.839020Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    #max_seq_length=512,\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_func,\n)\n\n# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=dataset,\n\n#     args=TrainingArguments(\n#         per_device_train_batch_size=2, \n#         gradient_accumulation_steps=8, \n#         warmup_steps=100, \n#         num_train_epochs=20, \n#         learning_rate=2e-4,\n#         fp16=True,  \n#         logging_steps=100,  \n#         save_steps=2000, \n#         save_total_limit=1,\n#         output_dir=\"/kaggle/working/outputs\",\n#         optim=\"adamw_torch\" \n#     ),\n#     peft_config=lora_config,\n#     formatting_func=formatting_func,\n# )\n\n#trainer.train()\n# #from transformers import DataCollatorForSeq2Seq\n\n# #data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=dataset,\n#     #data_collator=data_collator,\n#     args=TrainingArguments(\n#         per_device_train_batch_size=4,  # Adjust based on your hardware\n#         gradient_accumulation_steps=4,\n#         warmup_steps=250,  # Increase for smoother training\n#         num_train_epochs=2,  # Use multiple epochs instead of max_steps\n#         learning_rate=2e-4,  # Decrease learning rate for better convergence\n#         fp16=True,\n#         logging_steps=50,  # Log periodically to track progress\n#         save_steps=1000,  # Save checkpoints regularly\n#         save_total_limit=3,  # Limit the number of saved checkpoints\n#         output_dir=\"outputs\",\n#         optim=\"paged_adamw_8bit\"\n#     ),\n#     peft_config=lora_config,\n#     formatting_func=formatting_func,\n# )\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-01T16:47:26.841092Z","iopub.execute_input":"2025-02-01T16:47:26.841388Z","iopub.status.idle":"2025-02-01T16:56:15.897846Z","shell.execute_reply.started":"2025-02-01T16:47:26.841364Z","shell.execute_reply":"2025-02-01T16:56:15.896987Z"},"trusted":true},"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 1103, in emit\n    stream.write(msg + self.terminator)\nValueError: I/O operation on closed file\nCall stack:\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n    yield self.process_one()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n    self.do_execute(\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-0caa779ab157>\", line 5, in <cell line: 1>\n    args=TrainingArguments(\n  File \"<string>\", line 134, in __init__\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\", line 1861, in __post_init__\n    self.report_to = get_available_reporting_integrations()\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 601, in get_available_reporting_integrations\n    if is_wandb_available():\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 100, in is_wandb_available\n    logger.warning(\nMessage: 'Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).'\nArguments: ()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/52547 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4132523157f14e83b3969f2fd8378a60"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:56:15.898770Z","iopub.execute_input":"2025-02-01T16:56:15.899085Z","iopub.status.idle":"2025-02-01T17:04:29.544077Z","shell.execute_reply.started":"2025-02-01T16:56:15.899053Z","shell.execute_reply":"2025-02-01T17:04:29.543382Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 08:07, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.954400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.479300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.066000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.821800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.561700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.445400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.339700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.283900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.184100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.379800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.230600</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.334800</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.266400</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.219600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.180600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.172600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.376500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.418300</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.153600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.107100</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.232300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.120000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.204000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.076600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.040600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.152500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.062900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.055500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.227900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.208600</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.208000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.497000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.188400</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.168600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.112600</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.122100</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.207400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.117600</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.169300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.090900</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.120500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.199600</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.162100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.122700</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.043900</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.095900</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.150400</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.118900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.102300</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.086600</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.198000</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.114400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.050500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.130500</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.232800</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.091100</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.077300</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.104800</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.046900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.129000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.108900</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.063100</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.058000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.053200</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.190300</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.112800</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.135300</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.037600</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.060800</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.124600</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.074200</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.050900</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.140900</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.039500</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.088000</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.130900</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.099600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.1933905222825706, metrics={'train_runtime': 492.945, 'train_samples_per_second': 0.811, 'train_steps_per_second': 0.203, 'total_flos': 4706302911578112.0, 'train_loss': 0.1933905222825706, 'epoch': 7.150943396226415})"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"### The code above creates an instance of `SFTTrainer` from the Transformers library specifically designed for supervised fine-tuning (SFT) tasks. Here's a breakdown of what each part does:\n\n**SFTTrainer for Supervised Fine-Tuning**\n\nThis code utilizes `SFTTrainer` to fine-tune a pre-trained model (`model`) on a specific training dataset (`dataset`). It's designed for tasks where you have labeled data and want to adapt the model for a new purpose.\n\n**Key Parameters:**\n\n* **model (PreTrainedModel):** This argument specifies the pre-trained model you want to fine-tune.\n\n* **train_dataset (Dataset):** This argument points to the training dataset you'll use for fine-tuning. The dataset should be formatted appropriately for the task.\n\n* **max_seq_length (int):** This parameter defines the maximum sequence length allowed in the training data. Sequences exceeding this length will be truncated.\n\n* **args (TrainingArguments):** This argument is an instance of `TrainingArguments` that defines various hyperparameters for the training process. Here are some notable arguments within `args`:\n    * `per_device_train_batch_size (int)`: Sets the batch size per device (GPU/TPU) during training. Here, it's set to 1, which is a small batch size commonly used with gradient accumulation.\n    * `gradient_accumulation_steps (int)`: This parameter allows accumulating gradients over several batches before updating the model weights. Here, it's set to 4, effectively increasing the effective batch size.\n    * `warmup_steps (int)`: This defines the number of warmup steps where the learning rate is gradually increased from 0 to its full value. Here, it's set to 2.\n    * `max_steps (int)`: This parameter specifies the total number of training steps. Here, it's set to 50, which might be a short training run for fine-tuning depending on your dataset size and task complexity.\n    * `learning_rate (float)`: This sets the learning rate for the optimizer. Here, it's set to 2e-4, which is a common starting point for fine-tuning.\n    * `fp16 (bool)`: Enables training using 16-bit floating-point precision (mixed precision) for faster training with minimal accuracy loss (if supported by your hardware).\n    * `logging_steps (int)`: Defines how often training metrics are logged during training. Here, it's set to 1, logging metrics for every step. \n    * `output_dir (str)`: Specifies the directory where training outputs (model checkpoints, logs, etc.) will be saved. Here, it's set to \"outputs\".\n    * `optim (str)`: Defines the optimizer used for training. Here, it's set to \"paged_adamw_8bit\", which is likely an optimizer with specific memory optimizations. \n\n* **peft_config (LoraConfig):** This argument is likely referencing the `lora_config` you defined earlier. It provides the configuration for LoRA (Low-Rank Adaptation), which helps fine-tune the model more efficiently.\n\n* **formatting_func (Callable):** This argument (if provided) specifies a custom function for formatting the training data before feeding it to the model. This allows for specific pre-processing steps tailored to your task.\n\n**In essence:**\n\nThis code snippet configures and initializes an `SFTTrainer` for fine-tuning a pre-trained model with LoRA for memory efficiency. The training hyperparameters are set within the `TrainingArguments` object. ","metadata":{}},{"cell_type":"markdown","source":"## **Test the Fine-Tuned Model**","metadata":{}},{"cell_type":"code","source":"# system =  \"You are a skilled GPT.\"\n# question = \"What are the common syptoms of skin cancer?.\"\n# # system =  \"You are a skilled Medical GPT.\"\n# # question =system + \"Provide me detail information about drug. give me the answer through table?\"\n# prompt = f\"Question: {question} \\n Answer: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=512)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"Answer:\")[1])\n\n# Define system prompt and question\nsystem = \"You are a skilled GPT specialized in medical topics.\"\nquestion = \"What are the common symptoms of skin cancer?\"\n\n# Structured prompt\nprompt = f\"\"\"\n{system}\nPlease provide a well-organized, concise, and non-repetitive answer in bullet points.\n\nQuestion: {question}\nAnswer:\n\"\"\"\n\n# Tokenize input\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# Generate response\noutputs = model.generate(**inputs, num_return_sequences=1, max_length=300)\n\n# Decode and format output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract the answer after \"Answer:\" and remove duplicates\nanswer_section = text.split(\"Answer:\")[1].strip() if \"Answer:\" in text else text.strip()\n\n# Ensure the response is well-structured\nprint(\"\\n\".join(set(answer_section.split(\"\\n\"))))  # Removes duplicate lines\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:04:29.544948Z","iopub.execute_input":"2025-02-01T17:04:29.545291Z","iopub.status.idle":"2025-02-01T17:04:42.792886Z","shell.execute_reply.started":"2025-02-01T17:04:29.545231Z","shell.execute_reply":"2025-02-01T17:04:42.792011Z"}},"outputs":[{"name":"stdout","text":"\nSure, here are the common symptoms of skin cancer:\n- A mole that is located on the face, neck, or ears\n- Redness, irritation, or pain in the skin\n- New moles that appear anywhere on the body\n- A mole that is larger than 6 millimeters in diameter\n- Sores that don't heal\n- Changes in the skin's texture, such as a waxy or shiny appearance\n- A mole that is accompanied by a fever, night sweats, or other symptoms of a serious underlying condition\n- Thick, crusty, or bleeding skin\n- Changes in the size, shape, or color of existing moles\n- A change in the way a mole looks over time\n- A mole that is asymmetrical, meaning it has different colors or borders\n- A new mole that looks different from existing moles\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import torch\n\nsystem = \"You are a skilled  GPT.\"\n\n# Start an infinite loop for continuous questioning\nwhile True:\n    # Prompt the user for a question\n    user_question = input(\"Enter your question (or type 'exit' to stop): \")\n    \n    # Break the loop if the user wants to exit\n    if user_question.lower() == 'exit':\n        print(\"Exiting the question loop.\")\n        break\n    \n    # Construct the full question prompt\n    question = f\"{system} {user_question}\"\n    prompt = f\"Question: {question} \\n Answer: \"\n    \n    # Tokenize the input question\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    \n    # Generate the model's response\n    outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=512)\n    \n    # Decode and display the response\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = text.split(\"Answer:\")[1] if \"Answer:\" in text else text\n    print(\"\\nAnswer:\", answer, \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:04:42.793719Z","iopub.execute_input":"2025-02-01T17:04:42.793978Z","iopub.status.idle":"2025-02-01T17:58:43.273924Z","shell.execute_reply.started":"2025-02-01T17:04:42.793953Z","shell.execute_reply":"2025-02-01T17:58:43.272999Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  what is drug?\n"},{"name":"stdout","text":"\nAnswer:  \n\nA drug is a substance that is used to treat, cure, or prevent a disease, illness, or injury. It can be a natural product, a synthetic compound, or a combination of both. Drugs can be taken orally, intravenously, or topically. They can be used to treat a variety of conditions, including pain, nausea, vomiting, depression, anxiety, and insomnia. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  what is skin cancer?\n"},{"name":"stdout","text":"\nAnswer:  \n\nSkin cancer is a type of cancer that develops when cells in the skin change uncontrollably. These cells can grow out of control and spread to other parts of the body. Skin cancer is the most common cancer worldwide, and it is important to be aware of the signs and symptoms of skin cancer so that you can get it treated early.\n\n**Signs and symptoms of skin cancer include:**\n\n* A change in the size, shape, or color of a mole or other skin growth\n* A new mole or other skin growth that looks different from the rest of your skin\n* A sore or ulcer that doesn't heal\n* A rash that looks like a burn, even if it's not\n* A rash that appears in a new place or that changes in appearance\n* A rash that is asymmetrical, meaning it looks different on one side of the body than the other\n* A rash that is itchy, painful, or bleeding\n* A rash that is accompanied by a fever, night sweats, or weight loss\n* A rash that is caused by a chemical burn or exposure to radiation\n* A rash that is accompanied by a fever, night sweats, or weight loss\n* A rash that is caused by a virus\n* A rash that is caused by a drug or medication\n* A rash that is caused by a genetic disorder\n* A rash that is caused by a combination of factors, such as genetics, sun exposure, and certain medications\n\n**If you notice any of these signs and symptoms, it's important to see a doctor right away. Early diagnosis and treatment of skin cancer can help to improve your chances of survival. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  What is covid 19?\n"},{"name":"stdout","text":"\nAnswer:  \n\nCovid-19, also known as COVID-19, is a respiratory illness caused by the SARS-CoV-2 virus. It is a global health emergency, and has caused a significant impact on human health and economies worldwide. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  What is Abilify?\n"},{"name":"stdout","text":"\nAnswer:  \nAbilify is a medication that is used to treat attention deficit hyperactivity disorder (ADHD), a neurodevelopmental disorder characterized by difficulty paying attention, impulsivity, and hyperactivity. Abilify is a combination of two medications, guanfacine and atomoxetine, that work together to improve attention and focus. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  what are side effect of Abilify?\n"},{"name":"stdout","text":"\nAnswer:  \n\nAbilify is a medication that treats schizophrenia and other mental health disorders. It is a dopamine receptor antagonist, which means it blocks the effects of dopamine, a neurotransmitter that is involved in mood, movement, and reward. Abilify can cause a number of side effects, including:\n\n* Nausea\n* Vomiting\n* Diarrhea\n* Constipation\n* Weight loss\n* Increased heart rate\n* High blood pressure\n* Low blood pressure\n* Insomnia\n* Anxiety\n* Depression\n* Suicidal thoughts and behaviors\n* Weight gain\n* Muscle weakness\n* Difficulty sleeping\n* Irritability\n* Difficulty concentrating\n* Difficulty making eye contact\n* Blurred vision\n* Dizziness\n* Headache\n* Rash\n* Itching\n* Skin rash\n* Hair loss\n* Eye irritation\n* Nasal congestion\n* Sore throat\n* Headache\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness\n* Dizziness \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  how to prevent fever?\n"},{"name":"stdout","text":"\nAnswer:  \n\n**Fever prevention tips:**\n\n**1. Stay hydrated:** Drink plenty of fluids, especially water, throughout the day. Dehydration can contribute to a fever.\n\n**2. Get regular exercise:** Regular physical activity can help regulate your body temperature and improve your overall health.\n\n**3. Maintain a healthy diet:** Eating a balanced diet rich in fruits, vegetables, and whole grains can help boost your immune system.\n\n**4. Get enough sleep:** When you're sleep-deprived, your body is more susceptible to infection. Aim for 7-8 hours of sleep each night.\n\n**5. Avoid close contact with others:** If you're sick, avoid close contact with others to prevent spreading any germs.\n\n**6. Use a cool-mist humidifier:** A cool-mist humidifier can help relieve nasal congestion and soothe a sore throat, which can sometimes contribute to a fever.\n\n**7. Take over-the-counter medications:** Over-the-counter medications such as acetaminophen (Tylenol) or ibuprofen (Advil) can help reduce fever and pain.\n\n**8. Avoid smoking and alcohol:** Smoking and alcohol can irritate your throat and make you more susceptible to infection.\n\n**9. Seek medical attention:** If your fever is severe, persistent, or accompanied by other symptoms such as a rash, vomiting, or headache, seek medical attention immediately.\n\n**10. Follow your doctor's instructions:** Your doctor may recommend specific treatments or medications based on your individual situation. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  What is drug recommendation system?\n"},{"name":"stdout","text":"\nAnswer:  \n\nA drug recommendation system is a software program or algorithm that assists healthcare professionals in making informed decisions about which medications to prescribe to patients. It is designed to help identify patients who are most likely to respond to a particular treatment, and to provide them with a personalized list of potential medications that they may be suitable for. \n\nDrug recommendation systems typically use a variety of data sources, including:\n\n* Electronic health records (EHRs)\n* Clinical trial data\n* Drug manufacturer data\n* Patient demographics and medical history\n* Social media data\n\nThe system then uses machine learning algorithms to analyze this data and identify patterns that can help to predict which patients are more likely to respond to a particular treatment.\n\nOnce a list of potential medications has been generated, the system can be used by healthcare professionals to make informed decisions about which medications to prescribe to patients.\n\nDrug recommendation systems can be used by a variety of healthcare professionals, including:\n\n* Primary care physicians\n* Secondary care physicians\n* Hospitalists\n* Pharmacists\n* Nurses\n\nDrug recommendation systems can help to improve patient outcomes by reducing the risk of adverse events, and by ensuring that patients are receiving the most appropriate medications for their individual needs. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  What are the objectives of Drug recommendation system?\n"},{"name":"stdout","text":"\nAnswer:  \n\n**Objectives of a Drug Recommendation System (DRS):**\n\n**1. Improve Patient Safety and Drug Efficacy:**\n\n* Reduce the risk of adverse drug events (ADEs) by identifying patients who are most likely to experience drug-related problems.\n* Optimize medication regimens to minimize the risk of drug interactions and adverse drug reactions.\n* Enhance patient confidence and trust in the healthcare system by ensuring that they receive safe and effective medications.\n\n**2. Enhance Patient Engagement and Adherence:**\n\n* Promote patient education and self-management by providing personalized recommendations and reminders.\n* Facilitate communication between patients, healthcare providers, and pharmacists.\n* Reduce medication non-adherence and improve medication adherence.\n\n**3. Reduce Healthcare Costs:**\n\n* Identify patients who are at high risk of drug-related problems.\n* Reduce hospitalizations and emergency room visits associated with drug-related complications.\n* Optimize medication use and reduce unnecessary tests and procedures.\n\n**4. Improve Healthcare Provider Efficiency:**\n\n* Automate the identification of patients who are candidates for drug therapy.\n* Provide clinicians with evidence-based recommendations and insights.\n* Reduce the burden on healthcare providers by identifying patients who are unlikely to respond to specific medications.\n\n**5. Enhance Patient-Provider Relationships:**\n\n* Build trust and rapport between patients and healthcare providers.\n* Provide patients with clear and understandable information about their medications.\n* Facilitate open communication and shared decision-making.\n\n**6. Support Research and Development:**\n\n* Identify new drug candidates and targets.\n* Develop more effective and safe medications.\n* Improve the development of personalized medicine approaches.\n\n**7. Promote Patient Empowerment:**\n\n* Empower patients to make informed decisions about their health and medication management.\n* Provide patients with tools and resources to manage their medications.\n* Facilitate patient-centered care and shared decision-making.\n\n**8. Enhance Regulatory Compliance:**\n\n* Ensure that drug recommendations are safe and effective.\n* Reduce the risk of drug counterfeiting and adulteration.\n* Improve the quality of drug information and data. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  provide me some medicine for fever\n"},{"name":"stdout","text":"\nAnswer:  \n\nI'm not a medical professional, and I can't provide medical advice. If you're experiencing a fever, please consult a medical professional for proper diagnosis and treatment. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  exit\n"},{"name":"stdout","text":"Exiting the question loop.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"#  [](http://)**For Quick Response**","metadata":{}},{"cell_type":"code","source":"import torch\n\nsystem = \"You are a skilled Medical GPT.\"\n\n# Run an infinite loop for continuous questioning\nwhile True:\n    # Get user question\n    user_question = input(\"Enter your question (or type 'exit' to stop): \")\n    \n    # Exit loop if user wants to stop\n    if user_question.lower() == 'exit':\n        print(\"Exiting the question loop.\")\n        break\n    \n    # Construct the prompt\n    question = f\"{system} {user_question}\"\n    prompt = f\"Question: {question} \\n Answer: \"\n    \n    # Tokenize and prepare the input\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    \n    # Generate response with optimized settings\n    outputs = model.generate(\n        **inputs,\n        num_return_sequences=1,\n        max_new_tokens=100,          # Limit answer length\n        do_sample=False,             # Disable sampling for faster, deterministic responses\n        temperature=0.7               # Optional: lower temperature for shorter answers\n    )\n    \n    # Decode and display the response\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = text.split(\"Answer:\")[1] if \"Answer:\" in text else text\n    print(\"\\nAnswer:\", answer.strip(), \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:43.274981Z","iopub.execute_input":"2025-02-01T17:58:43.275351Z","iopub.status.idle":"2025-02-01T17:58:52.333594Z","shell.execute_reply.started":"2025-02-01T17:58:43.275316Z","shell.execute_reply":"2025-02-01T17:58:52.332803Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your question (or type 'exit' to stop):  exit\n"},{"name":"stdout","text":"Exiting the question loop.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# Gradio Interface of Fine Tune model","metadata":{}},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:52.334398Z","iopub.execute_input":"2025-02-01T17:58:52.334606Z","iopub.status.idle":"2025-02-01T17:58:55.853732Z","shell.execute_reply.started":"2025-02-01T17:58:52.334586Z","shell.execute_reply":"2025-02-01T17:58:55.852734Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.14.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.8)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\nRequirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.7.0)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import gradio as gr\nimport torch\n\n# Define system prompt\nsystem = \"You are a skilled Medical GPT.\"\n\n# Chatbot function\ndef chat(message, history):\n    \"\"\"Generates a response based on the user query and maintains chat history.\"\"\"\n    \n    # Construct the prompt with chat history for contextual replies\n    chat_history = \" \".join([f\"User: {h[0]}\\nBot: {h[1]}\" for h in history])\n    full_prompt = f\"{system}\\n{chat_history}\\nUser: {message}\\nBot:\"\n\n    # Tokenize input and move to GPU\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        num_return_sequences=1,\n        max_new_tokens=500,\n        do_sample=False,\n        temperature=0.7\n    )\n\n    # Decode response\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = text.split(\"Bot:\")[-1].strip()\n    \n    return response  # Gradio will automatically store history\n\n# Create ChatGPT-like interface\nchat_interface = gr.ChatInterface(fn=chat, title=\"Fine Tune Model Test\")\n\n# Launch the app\nchat_interface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:05:37.668020Z","iopub.execute_input":"2025-02-01T18:05:37.668350Z","iopub.status.idle":"2025-02-01T18:05:39.112581Z","shell.execute_reply.started":"2025-02-01T18:05:37.668322Z","shell.execute_reply":"2025-02-01T18:05:39.111941Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://200ce55038c6e045b8.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://200ce55038c6e045b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Upload finetune model to hugging face","metadata":{}},{"cell_type":"code","source":"# !pip install huggingface_hub --q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:57.451744Z","iopub.execute_input":"2025-02-01T17:58:57.451990Z","iopub.status.idle":"2025-02-01T17:58:57.455365Z","shell.execute_reply.started":"2025-02-01T17:58:57.451968Z","shell.execute_reply":"2025-02-01T17:58:57.454489Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# write= 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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:57.456346Z","iopub.execute_input":"2025-02-01T17:58:57.456601Z","iopub.status.idle":"2025-02-01T17:58:57.467255Z","shell.execute_reply.started":"2025-02-01T17:58:57.456564Z","shell.execute_reply":"2025-02-01T17:58:57.466451Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# import os\n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n\n# from huggingface_hub import login\n# login(os.getenv(\"HUGGINGFACE_TOKEN\"))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:57.468196Z","iopub.execute_input":"2025-02-01T17:58:57.468608Z","iopub.status.idle":"2025-02-01T17:58:57.480680Z","shell.execute_reply.started":"2025-02-01T17:58:57.468576Z","shell.execute_reply":"2025-02-01T17:58:57.479925Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# from huggingface_hub import whoami\n\n# user_info = whoami()\n# print(user_info)  # This will display your username and account details if logged in\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:57.481450Z","iopub.execute_input":"2025-02-01T17:58:57.481659Z","iopub.status.idle":"2025-02-01T17:58:57.492403Z","shell.execute_reply.started":"2025-02-01T17:58:57.481641Z","shell.execute_reply":"2025-02-01T17:58:57.491561Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# trainer.push_to_hub()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:58:57.496802Z","iopub.execute_input":"2025-02-01T17:58:57.497038Z","iopub.status.idle":"2025-02-01T17:58:57.503763Z","shell.execute_reply.started":"2025-02-01T17:58:57.497017Z","shell.execute_reply":"2025-02-01T17:58:57.503104Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:56:39.578195Z","iopub.execute_input":"2025-02-01T18:56:39.579119Z","iopub.status.idle":"2025-02-01T18:56:39.582854Z","shell.execute_reply.started":"2025-02-01T18:56:39.579085Z","shell.execute_reply":"2025-02-01T18:56:39.582003Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}